{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2e86c42-741b-487c-a3fa-3f636f62e6fe",
   "metadata": {},
   "source": [
    "# 수치/기초/기계 고장 여부 및 고장 범주 분류\n",
    "- 이 노트북은 셀을 차례로 실행하여 이미지 과제의 전반적인 과정을 수행해볼 수 있게 제작되었습니다.\n",
    "\n",
    "## 과제 설명\n",
    "- 기계 고장 예지 센서 데이터를 통해 각 기계가 정상인지 비정상인지, 만약 비정상이라면 어떠한 고장의 범주에 속하는지를 예측하는 과제\n",
    "\n",
    "## 데이터 설명\n",
    "- 입력 데이터 feature\n",
    "  - x, y, z : 3상 전류의 각 상\n",
    "- 출력 데이터 label\n",
    "  - 단위 시간당 수집된 센서 데이터의 오류 여부 및 오류의 종류이며 그 종류는 “정상”, “베어링 불량”, “회전체 불평형”, “축정렬 불량”, “벨트 느슨함”\n",
    "- 데이터셋 구성\n",
    "  - train: t(time), x, y, z 등의 feature 정보가 담긴 10000개의 csv 파일과 label 정보가 담긴 1개의 csv 파일(train.csv)\n",
    "  - test: t(time), x, y, z 등의 feature 정보가 담긴 2000개의 csv 파일\n",
    "  \n",
    "## 자주 사용되는 RNN 모델\n",
    "- LSTM, GRU 등\n",
    "\n",
    "## 코드 구조\n",
    "이 베이스라인 코드는 간단하게 아래 네 단계로 이루어져 있습니다.\n",
    "- `1.데이터`: 사용할 데이터셋을 가져오고 모델에 전달할 Dataloader 생성\n",
    "  - `class CustomDataset`: 데이터를 불러오고 (필요할 경우) 데이터 전처리 진행, 및 torch.utils.data.DataLoader의 첫번째 인자 형식으로 변환\n",
    "  - `torch.utils.data.DataLoader(dataset, batch_size=, ...)`: 모델에 공급할 데이터 로더 생성\n",
    "- `2.모델 설계`: 학습 및 추론에 사용할 모델 구조 설계\n",
    "  - `class LSTMClassifier`: 모델 구조 설계\n",
    "- `3.학습`: 설계된 모델로 데이터 학습\n",
    "  - 학습된 모델은 `.ipynb` 코드와 같은 경로에 저장됨\n",
    "- `4.추론`: 학습된 모델을 사용해 테스트 데이터로 추론\n",
    "  - 학습된 모델로 테스트 데이터에 대한 추론을 진행\n",
    "  - 추론 결과는 `.ipynb` 코드와 같은 경로에 저장됨. 이를 플랫폼에 업로드해 점수 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4bcdbf5-6d34-426c-9a8c-f92fc46cdd8a",
   "metadata": {},
   "source": [
    "## 세팅\n",
    "### 라이브러리\n",
    "- 코드 전반에 사용되는 라이브러리를 설치 및 로드합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8082ee1b-9bcd-449a-9fe5-307713810b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 설치되지 않은 라이브러리의 경우, 주석 해제 후 코드를 실행하여 설치\n",
    "# !pip install torch\n",
    "# !pip install "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a325ee8d-d888-4905-8d4e-d09fb4bb0108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 라이브러리 불러오기\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37049c4a-69d2-4829-a806-a3e5fdc3339c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "_CudaDeviceProperties(name='Tesla V100-PCIE-32GB', major=7, minor=0, total_memory=32480MB, multi_processor_count=80)\n"
     ]
    }
   ],
   "source": [
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "if device == torch.device('cuda'):\n",
    "    print( torch.cuda.get_device_properties( device ))\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\"\n",
    "\n",
    "# 경로 설정\n",
    "DATA_DIR = '/workspace/01_data/18_machine/02_processed/d3'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc44da4-9e71-41fe-b8b0-c8c0f585fcfb",
   "metadata": {},
   "source": [
    "## 데이터 및 전처리\n",
    "- file name과 label 정보가 담긴 meta data(train.csv)를 통해 개별 기계시설물 csv에 맵핑하여 데이터 로드 작업 수행\n",
    "- 10000개의 train dataset을 train / val set(8:2의 비율)으로 분할 \n",
    "- 총 5개 범주에 대한 인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7240b5b9-e736-4076-975b-04b56725047c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation set ratio\n",
    "VAL_RATIO = 0.2\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data_dir, data_meta, mode, sequence_length=2000):\n",
    "        \"\"\"\n",
    "        data_meta : train.csv, test.csv etc. This is the file that contains file_name, label, etc.\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        self.data_dir = os.path.join(data_dir, 'train')\n",
    "        self.data_meta = os.path.join(data_dir, data_meta)\n",
    "        self.mode = mode\n",
    "        self.seq_len = sequence_length # 데이터 파일 하나당 행 갯수\n",
    "        self.label_onehot = {\n",
    "            '축정렬불량': 0,\n",
    "            '회전체불평형': 1,\n",
    "            '베어링불량': 2,\n",
    "            '벨트느슨함': 3,\n",
    "            '정상': 4,\n",
    "        }\n",
    "        self.db, self.label = self.data_loader()\n",
    "\n",
    "        \n",
    "    def data_loader(self):\n",
    "        \n",
    "        # 파일 없을때 뜨는 메세지\n",
    "        print('Loading ' + self.mode + ' dataset..')\n",
    "        if not os.path.isdir(self.data_dir):\n",
    "            print(f'!!! Cannot find {self.data_dir}... !!!')\n",
    "            sys.exit()\n",
    "        if not os.path.lexists(self.data_meta):\n",
    "            print(f'!!! Cannot find {self.data_meta}... !!!')\n",
    "            sys.exit()\n",
    "            \n",
    "        # 일단 클래스별로 나열돼있음.\n",
    "        meta = pd.read_csv(self.data_meta)\n",
    "        \n",
    "        # 학습 데이터 클래스 상관 없이 섞어버리기\n",
    "        meta = meta.sample(frac=1).reset_index(drop=True) \n",
    "\n",
    "        # db_train, db_val 선언\n",
    "        db_train = db_val= pd.DataFrame(columns=['file_name','label'])\n",
    " \n",
    "        for la in self.label_onehot.keys():\n",
    "            df = meta[meta['label']==la]\n",
    "            \n",
    "            # 학습, 검증 데이터 분할\n",
    "            train, val = train_test_split(df, test_size=VAL_RATIO, random_state=42, shuffle=True)\n",
    "            \n",
    "            db_train = pd.concat([db_train,train])\n",
    "            db_val = pd.concat([db_val,val])\n",
    "        \n",
    "        db_train = db_train.sample(frac=1).reset_index(drop=True)\n",
    "        db_val = db_val.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "       \n",
    "        if self.mode=='train':\n",
    "            db = db_train\n",
    "            \n",
    "            # label 숫자값으로 바꾸기\n",
    "            label = pd.DataFrame(db_train['label'].map(self.label_onehot), columns=['label'])\n",
    "            \n",
    "        elif self.mode=='val':\n",
    "            db = db_val\n",
    "            \n",
    "            # label 숫자값으로 바꾸기\n",
    "            label = pd.DataFrame(db_val['label'].map(self.label_onehot), columns=['label'])\n",
    "        else:\n",
    "            print(\"Please check your mode : \", mode, \" must be either train or val\")\n",
    "        \n",
    "\n",
    "        # db returns something like train.csv (filename, label(str), rms, ....)\n",
    "        # label returns label(integer) dataframe       \n",
    "        return db, label\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.label)\n",
    "    \n",
    "    # idx(파일 넘버)에 관한 정보 추출\n",
    "    def __getitem__(self, idx):\n",
    "        filename = self.db.iloc[idx]['file_name']\n",
    "        label_folder = self.db.iloc[idx]['label']\n",
    "        \n",
    "        file_path = os.path.join(self.data_dir, label_folder, filename)\n",
    "        data = pd.read_csv(file_path)\n",
    "        label = self.label.iloc[idx]['label']\n",
    "        # t는 시간이므로 feature가 아니라 제거\n",
    "        data = data.drop(columns=['t']).values.astype('float32')\n",
    "        \n",
    "        return_dict = {'data': data, 'label': label}\n",
    "        return return_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "37981054-56ae-418b-936f-a3914eede89c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train dataset..\n",
      "Loading val dataset..\n"
     ]
    }
   ],
   "source": [
    "trainset = CustomDataset(DATA_DIR, \"train.csv\", 'train')\n",
    "valset = CustomDataset(DATA_DIR, \"train.csv\", 'val')\n",
    "\n",
    "batch_size=64\n",
    "trainloader = DataLoader(trainset, shuffle=True, batch_size=batch_size)\n",
    "valloader = DataLoader(valset, shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f54c6ce-862d-46b8-a193-36b594ca4e43",
   "metadata": {},
   "source": [
    "## 모델 설계\n",
    "### 사용할 파라미터\n",
    "- `LEARNING_RATE` : 경사하강법(Gradient Descent)을 통해 loss function의 minimum값을 찾아다닐 때, 그 탐색 과정에 있어서의 보폭 정도로 직관적으로 이해 할 수 있습니다. 보폭이 너무 크다면 최적값을 쉽게 지나칠 위험이 있고, 보폭이 너무 작다면 탐색에 걸리는 시간이 길어집니다.\n",
    "- `EPOCHS` : \n",
    "  - 한 번의 epoch는 인공 신경망에서 전체 데이터 셋에 대해 forward pass/backward pass 과정을 거친 것입니다.\n",
    "  - 즉, epoch이 1만큼 지나면, 전체 데이터 셋에 대해 한번의 학습이 완료된 상태입니다.\n",
    "  - 모델을 만들 때 적절한 epoch 값을 설정해야만 underfitting과 overfitting을 방지할 수 있습니다.\n",
    "  - 1 epoch = (데이터 갯수 / batch size) interations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "de341b90-1013-41a9-aae2-583d41074215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper-parameters\n",
    "lr = 0.001\n",
    "n_epochs = 10\n",
    "iterations_per_epoch = len(trainloader)\n",
    "best_acc = 0\n",
    "patience, patience_counter = 50, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3afc478a-fb35-49e3-9911-28d88eb4cdd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=3, hidden_dim=256, num_layers=2, output_dim=5, dropout=0):\n",
    "        \"\"\"\n",
    "        input_dim = number of features at each time step \n",
    "                    (number of features given to each LSTM cell)\n",
    "        hidden_dim = number of features produced by each LSTM cell (in each layer)\n",
    "        num_layers = number of LSTM layers\n",
    "        output_dim = number of classes of motor anomaly \n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_dim, \n",
    "                            num_layers=num_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "        \n",
    "    def forward(self, X):\n",
    "        hidden_features, (h_n, c_n) = self.lstm(X)  # (h_0, c_0) default to zeros\n",
    "        hidden_features = hidden_features[:,-1,:]  # index only the features produced by the last LSTM cell\n",
    "        out = self.fc(hidden_features)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6bb9ace7-f4aa-4dbf-8f86-ed0521dc99aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMClassifier(dropout=0.75)\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0896e716-2de1-4df7-9aff-038e2220a634",
   "metadata": {},
   "source": [
    "## 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "acd04d31-d931-498f-8f0c-e1c89ce6bbf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start model training\n",
      "Epoch 1 best model saved with accuracy: 83.85%\n",
      "Epoch:   5. Train Loss: 0.9970. Val Loss: 1.3440 Acc.: 41.80%\n",
      "Epoch 7 best model saved with accuracy: 86.55%\n",
      "Epoch:  10. Train Loss: 1.4494. Val Loss: 1.4867 Acc.: 29.20%\n"
     ]
    }
   ],
   "source": [
    "print('Start model training')\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    \n",
    "    # initialize losses\n",
    "    loss_train_total = 0\n",
    "    loss_val_total = 0\n",
    "    \n",
    "    # Training loop\n",
    "    for i, batch_data in enumerate(trainloader):\n",
    "        model.train()\n",
    "        X_batch = batch_data['data'].to(device)\n",
    "        y_batch = batch_data['label'].to(device).long()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        y_pred = model(X_batch) \n",
    "        loss = criterion(y_pred, y_batch)\n",
    "        loss_train_total += loss.cpu().detach().item() * batch_size\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    loss_train_total = loss_train_total / len(trainset)\n",
    "    \n",
    "    \n",
    "    # Validation loop\n",
    "    with torch.no_grad():\n",
    "        for i, batch_data in enumerate(valloader):\n",
    "            model.eval()\n",
    "            X_batch = batch_data['data'].to(device)\n",
    "            y_batch = batch_data['label'].to(device).long()\n",
    "\n",
    "            y_pred = model(X_batch)\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            loss_val_total += loss.cpu().detach().item() * batch_size\n",
    "\n",
    "    loss_val_total = loss_val_total / len(valset)\n",
    "    \n",
    "    \n",
    "    # Validation Accuracy\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for i, batch_data in enumerate(valloader):\n",
    "            X_batch = batch_data['data'].to(device)\n",
    "            y_batch = batch_data['label'].to(device).long()\n",
    "\n",
    "            y_pred = model(X_batch)\n",
    "            class_predictions = F.log_softmax(y_pred, dim=1).argmax(dim=1)\n",
    "            total += y_batch.size(0)\n",
    "            correct += (class_predictions == y_batch).sum().item()\n",
    "\n",
    "    acc = correct / total\n",
    "\n",
    "    \n",
    "    # Logging\n",
    "    if epoch % 5 == 0:\n",
    "        print(f'Epoch: {epoch:3d}. Train Loss: {loss_train_total:.4f}. Val Loss: {loss_val_total:.4f} Acc.: {acc:2.2%}')\n",
    "\n",
    "    if acc > best_acc:\n",
    "        patience_counter = 0\n",
    "        best_acc = acc\n",
    "        torch.save(model.state_dict(), 'best_211101.pth')\n",
    "        print(f'Epoch {epoch} best model saved with accuracy: {best_acc:2.2%}')\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f'Early stopping on epoch {epoch}')\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d6df6a-94f8-4eeb-865c-c1c57bacfb8b",
   "metadata": {},
   "source": [
    "# 추론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7fd97fbf-e1f2-45f9-8fa8-f4fdd59a7938",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '/workspace/01_data/18_machine/02_processed/d3/'\n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, data_dir, sequence_length=2000):\n",
    "        \"\"\"\n",
    "        data_meta : train.csv. This is the file that contains file_name, label, etc.\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        self.data_dir = os.path.join(data_dir, 'test')\n",
    "        self.seq_len = sequence_length # 데이터 파일 하나당 행 갯수\n",
    "        self.db = self.test_loader()\n",
    "\n",
    "\n",
    "    def test_loader(self):\n",
    "       \n",
    "        # 파일 없을때 뜨는 메세지\n",
    "        print('Loading ' + 'test' + ' dataset..')\n",
    "        if not os.path.isdir(self.data_dir):\n",
    "            print(f'!!! Cannot find {self.data_dir}... !!!')\n",
    "            sys.exit()\n",
    "\n",
    "        db_test = os.listdir(self.data_dir)\n",
    "        if '.ipynb_checkpoints' in db_test:\n",
    "            db_test.remove('.ipynb_checkpoints')\n",
    "        db_test = pd.DataFrame(db_test, columns=['file_name'])\n",
    "\n",
    "\n",
    "        # db returns something like train.csv (filename, label(str), rms, ....)\n",
    "        # label returns label(integer) dataframe       \n",
    "        return db_test\n",
    "  \n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.db)\n",
    "\n",
    "    \n",
    "    # idx(파일 넘버)에 관해 정보 뽑아냄\n",
    "    def __getitem__(self, idx):\n",
    "        filename = self.db.iloc[idx]['file_name']\n",
    "        file_path = os.path.join(self.data_dir, filename)\n",
    "        data = pd.read_csv(file_path)\n",
    "        # t 는 시간이므로 feature가 아니라 제거\n",
    "        data = data.drop(columns=['t']).values.astype('float32')\n",
    "        \n",
    "        return_dict = {'file_name':filename,'data': data}\n",
    "        return return_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fc45978f-c14f-4e9f-9e53-eacab54843a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test dataset..\n"
     ]
    }
   ],
   "source": [
    "testset = TestDataset(DATA_DIR, 'test')\n",
    "\n",
    "batch_size= 64\n",
    "testloader = DataLoader(testset, shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "edf422e4-3f6c-4bce-b3e4-85c32f50c4c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LSTMClassifier(dropout=0.75)\n",
    "model.to(device)\n",
    "model.load_state_dict(torch.load('best_211101.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1e573038-b8a7-4d26-bf18-af513722dcc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction start\n",
      "prediction finished\n"
     ]
    }
   ],
   "source": [
    "pred_list = []\n",
    "filename_list = []\n",
    "with torch.no_grad():\n",
    "    print('prediction start')\n",
    "    model.eval()\n",
    "    for i, batch_data in enumerate(testloader):\n",
    "        X_batch = batch_data['data'].to(device)\n",
    "\n",
    "        y_pred = model(X_batch)\n",
    "        class_predictions = F.log_softmax(y_pred, dim=1).argmax(dim=1).tolist()\n",
    "        pred_list.extend(class_predictions)\n",
    "        filename_list.extend(batch_data['file_name'])\n",
    "print(\"prediction finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2be6219f-243b-4cf8-9f3e-c38b29ce3f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 추론 결과디코딩\n",
    "label_dict = {\n",
    "            '축정렬불량': 0,\n",
    "            '회전체불평형': 1,\n",
    "            '베어링불량': 2,\n",
    "            '벨트느슨함': 3,\n",
    "            '정상': 4,\n",
    "        }\n",
    "decode = dict(map(reversed, label_dict.items()))\n",
    "pred_list = list(map(lambda x: decode[x], pred_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "424572f2-bbb5-4883-bfbd-bda3ca4b8ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'file_name':filename_list, 'label':pred_list})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a1d5b1e4-3bfe-4861-b5b2-3abbb82bfdc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.csv</td>\n",
       "      <td>벨트느슨함</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.csv</td>\n",
       "      <td>벨트느슨함</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.csv</td>\n",
       "      <td>축정렬불량</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.csv</td>\n",
       "      <td>벨트느슨함</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.csv</td>\n",
       "      <td>회전체불평형</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  file_name   label\n",
       "0     0.csv   벨트느슨함\n",
       "1     1.csv   벨트느슨함\n",
       "2     2.csv   축정렬불량\n",
       "3     3.csv   벨트느슨함\n",
       "4     4.csv  회전체불평형"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1916d4b9-8e0f-4b39-b376-85237099b2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 제출 파일 제작\n",
    "df.to_csv(\"/workspace/01_data/18_machine/02_processed/d3/prediction_211101.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
